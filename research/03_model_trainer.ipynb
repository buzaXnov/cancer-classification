{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/filip/Documents/cancer-classification/research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/filip/Documents/cancer-classification'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    root_dir: Path\n",
    "    trained_model_path: Path\n",
    "    updated_model_path: Path\n",
    "    training_data: Path\n",
    "    checkpoints_dir: Path\n",
    "    params_epochs: int\n",
    "    params_batch_size: int\n",
    "    params_augmentation: bool\n",
    "    params_image_size: list\n",
    "    params_learning_rate: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/filip/Documents/cancer-classification/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "from cnnClassifier.constants import CONFIG_FILE_PATH, PARAMS_FILE_PATH\n",
    "from cnnClassifier.utils.common import read_yaml, create_directories\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision as torchvision\n",
    "from torchvision import transforms\n",
    "import torch.utils.data as data\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self, config_filepath=CONFIG_FILE_PATH, params_filepath=PARAMS_FILE_PATH\n",
    "    ) -> None:\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_training_config(self) -> TrainingConfig:\n",
    "        training = self.config.training\n",
    "        prepare_base_model = self.config.prepare_base_model\n",
    "        params = self.params\n",
    "        training_data = os.path.join(self.config.data_ingestion.unzip_dir, \"Data\")\n",
    "        checkpoints_dir = os.path.join(training.checkpoints_dir)\n",
    "        create_directories([Path(training.root_dir), checkpoints_dir])\n",
    "\n",
    "        training_config = TrainingConfig(\n",
    "            root_dir=Path(training.root_dir),\n",
    "            trained_model_path=Path(training.trained_model_path),\n",
    "            updated_model_path=Path(prepare_base_model.updated_base_model_path),\n",
    "            training_data=Path(training_data),\n",
    "            checkpoints_dir=Path(checkpoints_dir),\n",
    "            params_image_size=params.IMAGE_SIZE,\n",
    "            params_augmentation=params.AUGMENTATION,\n",
    "            params_batch_size=params.BATCH_SIZE,\n",
    "            params_epochs=params.EPOCHS,\n",
    "            params_learning_rate=params.LEARNING_RATE\n",
    "        )\n",
    "\n",
    "        return training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training:\n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config: TrainingConfig = config\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = None\n",
    "\n",
    "    def get_base_model(self):\n",
    "        \"\"\"Download the base model.\"\"\"\n",
    "        self.model = torch.load(self.config.updated_model_path).to(self.device)\n",
    "        # NOTE: model.eval() for inference later on, not before training\n",
    "\n",
    "    def get_dataloaders(self):\n",
    "        \"\"\"Create dataloaders for the training loop with the appropriate augmentations.\"\"\"\n",
    "        # TRAIN_DATA_PATH = \"ImageFolder/images/train/\"; self.config.training_data\n",
    "        TRAIN_DATA_PATH = os.path.join(self.config.training_data, \"train\")\n",
    "        VAL_DATA_PATH = os.path.join(self.config.training_data, \"valid\")\n",
    "        TEST_DATA_PATH = os.path.join(self.config.training_data, \"test\")\n",
    "\n",
    "        if self.config.params_augmentation:\n",
    "            transform_img = transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize(256),\n",
    "                    transforms.CenterCrop(256),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(\n",
    "                        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            transform_img = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(\n",
    "                        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        # Datasets\n",
    "        train_dataset = torchvision.datasets.ImageFolder(\n",
    "            root=TRAIN_DATA_PATH, transform=transform_img\n",
    "        )\n",
    "        val_dataset = torchvision.datasets.ImageFolder(\n",
    "            root=VAL_DATA_PATH, transform=transform_img\n",
    "        )\n",
    "        test_dataset = torchvision.datasets.ImageFolder(\n",
    "            root=TEST_DATA_PATH, transform=transform_img\n",
    "        )\n",
    "\n",
    "        # Dataloaders\n",
    "        BATCH_SIZE = self.config.params_batch_size\n",
    "        self.train_loader = data.DataLoader(\n",
    "            dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4\n",
    "        )\n",
    "        self.val_loader = data.DataLoader(\n",
    "            dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4\n",
    "        )\n",
    "        self.test_loader = data.DataLoader(\n",
    "            dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4\n",
    "        )\n",
    "\n",
    "    def save_checkpoint(self, epoch, optimizer, scheduler, val_accuracy):\n",
    "        \"\"\"Save the model checkpoint.\"\"\"\n",
    "        state = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": self.model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "            \"val_accuracy\": val_accuracy,\n",
    "        }\n",
    "        checkpoint_path = os.path.join(\n",
    "            self.config.checkpoints_dir, f\"checkpoint_epoch_{epoch}.pth\"\n",
    "        )\n",
    "        torch.save(state, checkpoint_path)\n",
    "        print(\n",
    "            f\"Checkpoint saved at epoch {epoch} with validation accuracy: {val_accuracy}\"\n",
    "        )\n",
    "\n",
    "    def load_checkpoint(self, checkpoint_path):\n",
    "        \"\"\"Load a model checkpoint.\"\"\"\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        self.model.to(self.device)\n",
    "        return (\n",
    "            checkpoint[\"epoch\"],\n",
    "            checkpoint[\"optimizer_state_dict\"],\n",
    "            checkpoint[\"scheduler_state_dict\"],\n",
    "            checkpoint[\"val_accuracy\"],\n",
    "        )\n",
    "\n",
    "    def train(self):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(), lr=self.config.params_learning_rate\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "        num_epochs = self.config.params_epochs\n",
    "\n",
    "        best_val_accuracy = 0.0\n",
    "        for epoch in tqdm(range(num_epochs)):\n",
    "            self.model.train()\n",
    "            running_loss = 0.0\n",
    "            \n",
    "            for inputs, labels in tqdm(self.train_loader):\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "\n",
    "                # zero the parameters gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = self.model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(self.train_loader)}\")\n",
    "\n",
    "            # validation step\n",
    "            valid_accuracy = self.validate()\n",
    "\n",
    "            if valid_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = valid_accuracy\n",
    "                self.save_checkpoint(epoch+1, optimizer, scheduler, best_val_accuracy)\n",
    "\n",
    "            # Step the scheduler\n",
    "            scheduler.step()\n",
    "        \n",
    "\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        total, correct = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(self.val_loader):\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        val_accuracy = 100 * correct / total\n",
    "        print(f\"Validation Accuracy: {val_accuracy}%\")\n",
    "        return val_accuracy\n",
    "\n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        total, correct = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(self.test_loader):\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        test_accuracy = 100 * correct / total\n",
    "        print(f\"Test Accuracy: {test_accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-14 16:58:24,463: INFO: common: YAML file: config/config.yaml loaded successfully!]\n",
      "[2024-05-14 16:58:24,464: INFO: common: YAML file: params.yaml loaded successfully!]\n",
      "[2024-05-14 16:58:24,465: INFO: common: Created directory at: artifacts]\n",
      "[2024-05-14 16:58:24,466: INFO: common: Created directory at: artifacts/training]\n",
      "[2024-05-14 16:58:24,466: INFO: common: Created directory at: artifacts/training/checkpoints]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [01:34<00:00,  2.43s/it]\n",
      "  0%|          | 0/1 [01:34<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Loss: 1.0602650076915057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Training.validate() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# training.validate()\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[0;32mIn[16], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m     training\u001b[38;5;241m.\u001b[39mget_base_model()\n\u001b[1;32m      6\u001b[0m     training\u001b[38;5;241m.\u001b[39mget_dataloaders()\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mtraining\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# training.validate()\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[15], line 122\u001b[0m, in \u001b[0;36mTraining.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# validation step\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m valid_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid_accuracy \u001b[38;5;241m>\u001b[39m best_val_accuracy:\n\u001b[1;32m    125\u001b[0m     best_val_accuracy \u001b[38;5;241m=\u001b[39m valid_accuracy\n",
      "\u001b[0;31mTypeError\u001b[0m: Training.validate() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    training_config = config.get_training_config()\n",
    "    training = Training(training_config)\n",
    "    training.get_base_model()\n",
    "    training.get_dataloaders()\n",
    "    training.train()\n",
    "    training.test()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
